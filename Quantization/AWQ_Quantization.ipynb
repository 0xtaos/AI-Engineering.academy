{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qunatize with AWQ\n",
    "\n",
    "Activation Aware Quantization\n",
    "\n",
    "This notebook is for you to qunatize huggingface models in AWQ formate and upload them to the Hub\n",
    "\n",
    "[Paper](https://arxiv.org/abs/2306.00978)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install from source \n",
    "\n",
    "# !git clone https://github.com/casper-hansen/AutoAWQ\n",
    "# %cd AutoAWQ\n",
    "# !pip install -e .\n",
    "# %cd ..\n",
    "\n",
    "\n",
    "# quick install the most stable version\n",
    "!pip install autoawq -q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install transformers from the source - dev version\n",
    "!pip install  git+https://github.com/huggingface/transformers.git -q\n",
    "!pip install huggingface_hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awq import AutoAWSForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import torch\n",
    "\n",
    "model_path = \"\"\n",
    "\n",
    "quant_name =  model_path.split(\"/\")[-1] + \"-AWQ\"\n",
    "\n",
    "quant_path = \"AdithyaSK/\" + quant_name\n",
    "quant_config = {\"zero_point\" : True, \"q_group_size\":128,\"w_bit\":4}\n",
    "\n",
    "#Load model\n",
    "model = AutoAWSForCausalLM.from_pretrained(model_path , device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code = True)\n",
    "\n",
    "# Quantize \n",
    "\n",
    "model.quantize(tokenizer,quant_config=quant_config)\n",
    "\n",
    "model.save(quant_name, safetensors=True , shard_size=\"10GB\")\n",
    "tokenizer.save_pretrained(quant_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Push models and tokenizers to Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hugginface_hub import HfApi\n",
    "\n",
    "api = HfApi()\n",
    "\n",
    "path_in_repo = \"model.safetensors\"\n",
    "\n",
    "local_file_path = \"./\"+ quant_name + \"/\" + path_in_repo\n",
    "\n",
    "repo_id = \"AdithyaSK/\" + quant_name\n",
    "\n",
    "api.upload_file(\n",
    "    path_or_fileobj = local_file_path,\n",
    "    path_in_repo = path_in_repo,\n",
    "    repo_id = repo_id,\n",
    "    repo_type = \"model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload non-Model Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "api = HfApi()\n",
    "\n",
    "repo_id = \"AdithyaSK/\"+ quant_name\n",
    "\n",
    "local_file_paths = [\n",
    "    \"./\"+ quant_name + \"/config.json\",\n",
    "    \"./\"+ quant_name + \"/pytorch_model.bin\",\n",
    "    \"./\"+ quant_name + \"/special_tokens_map.json\",\n",
    "    \"./\"+ quant_name + \"/tokenizer_config.json\",\n",
    "    \"./\"+ quant_name + \"/tokenizer.json\",\n",
    "]\n",
    "\n",
    "#Loop thorugh each file and upload\n",
    "for local_file_path in local_file_paths:\n",
    "    file_name = local_file_path.split(\"/\")[-1]\n",
    "    \n",
    "    path_in_repo = file_name\n",
    "    \n",
    "    api.upload_file(\n",
    "        path_or_fileobj=local_file_path,\n",
    "        path_in_repo=path_in_repo,\n",
    "        repo_id=repo_id,\n",
    "        repo_type=\"model\"\n",
    "    )\n",
    "    print(f\"Uploaded {file_name} to {repo_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run AWQ Inference with AutoAWQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load AWQ Model\n",
    "\n",
    "from awq import AutoAWQForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "### NOte that the model must be in safetensors formate!\n",
    "\n",
    "# model_name_or_path = \"TheBloke/Llama-2-7b-Chat-AWQ\"\n",
    "model_name_or_path = \"AdithyaSK/\"\n",
    "\n",
    "model = AutoAWQForCausalLM.from_quantized(model_name_or_path, fuse_layer=True,trust_remote_code = False, safetensors = True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Load model in bf16\n",
    "\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# import torch\n",
    "\n",
    "# model_name_or_path = \"\" # model name \n",
    "\n",
    "# ## Load model\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name_or_path, torch_dtype=torch.float16 , trust_remote_code = True , device =\"cuda\")\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Who played the character Iron man?\"\n",
    "\n",
    "fromatted_prompt = f\"<|im_start|>users\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "\n",
    "tokens = tokenizer(fromatted_prompt,return_tensors=\"pt\").input_ids.cuda()\n",
    "\n",
    "# Generate Output\n",
    "\n",
    "generation_output = model.generate(tokens,do_sample=False,max_new_tokes=512)\n",
    "\n",
    "print(tokenizer.decode(generation_output[0],skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
