{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qunatize with GGUF\n",
    "\n",
    "This notebook is for you to qunatize huggingface models in GGUF formate and upload them to the Hub\n",
    "GGUF fromat is to run models on the CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Login to Huggingface Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "noteboo_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = ''\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cache_dir = \"/content/drive/My Drive/huggingface_cache\"\n",
    "os.makedirs(cache_dir, exist_ok=True) # Ensure directory exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import locale\n",
    "\n",
    "def getpreferredencoding(do_setlocale = True):\n",
    "    return \"UTF-8\"\n",
    "\n",
    "locale.getpreferredencoding = getpreferredencoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "!pip install -q -U git+https://github.com/huggingface/acclerate.git\n",
    "!pip install -q -U einops\n",
    "!pip install numpy==1.24\n",
    "!pip install sentencepiece==0.1.98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoConfig , AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"meta-lloma/Llama-2-7b\"\n",
    "model_name = \"\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    trust_remote_code = True,\n",
    "    torch_dtype = torch.bfloat16,\n",
    "    device_map=\"cpu\",\n",
    "    offload_folder=\"offload\",\n",
    "    cache_dir = cache_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd cache_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/ggerganov/llama.cpp.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./models/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Tokenizer config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "def download_file_from_huggingface(\n",
    "    model_name,\n",
    "    filename,\n",
    "    save_path\n",
    "):\n",
    "    url = f\"https://huggingface.co/{model_name}/resolve/main/{filename}\"\n",
    "    r = requests.get(url, allow_redirects=True)\n",
    "    if r.status_code != 200:\n",
    "        print(f\"Error downloading {filename}. HTTP Status Code: {r.status_code}\")\n",
    "        return False\n",
    "    with open(os.path.join(save_path, filename), \"wb\") as f:\n",
    "        f.write(r.content)\n",
    "    return True\n",
    "\n",
    "def main():\n",
    "    files_to_download = [\n",
    "        \"tokenizer.json\",\n",
    "        \"tokenizer.model\",\n",
    "        \"tokenizer_config.json\",\n",
    "        \"special_tokens_map.json\",\n",
    "        \"added_tokens_map.json\",\n",
    "    ]\n",
    "    \n",
    "    save_path = \"./models/\"\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "        \n",
    "    for filename in files_to_download:\n",
    "        success = download_file_from_huggingface(model_name, filename, save_path)\n",
    "        if success:\n",
    "            print(f\"Successfully downloaded {filename}\")\n",
    "        else:\n",
    "            print(f\"Error downloading {filename}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting to GGUF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt update -y\n",
    "!apt install build-essential git cmake libopenblas-dev libeigen3-dev -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!make LLAMA_OPENBLAS=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python convert.py models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parts = model_name.split('/')\n",
    "\n",
    "model_name_pure = parts[1]\n",
    "\n",
    "quant_type = \"Q4_K\"\n",
    "quantized_model = f'models/{model_name_pure}.{quant_type}.gguf'\n",
    "print(f'Preparing {quantized_model} with {quant_type} quantization.')\n",
    "\n",
    "import subprocess\n",
    "\n",
    "command = [\"./quantize\",\"models/ggml-model-f16.gguf\",quantized_model,quant_type]\n",
    "\n",
    "subprocess.run(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Push to Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "api = HfApi()\n",
    "\n",
    "quant_name = model_name.split('/')[-1] + \"-GGUF\"\n",
    "\n",
    "repo_id = \"AdithyaSK/\"+ quant_name\n",
    "\n",
    "base_path = \"./models\"\n",
    "\n",
    "local_file_paths = [\n",
    "    base_path + \"/tokenizer.json\",\n",
    "    base_path + \"/tokenizer.model\",\n",
    "    base_path + \"/tokenizer_config.json\",\n",
    "    base_path + \"/special_tokens_map.json\",\n",
    "    base_path + \"/ggml-vocab-llama.gguf\",\n",
    "    base_path + \"/\" + f'{model_name_pure}.{quant_type}.gguf',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for local_file_path in local_file_paths:\n",
    "    \n",
    "    file_name = local_file_paths.split(\"/\")[-1]\n",
    "    \n",
    "    path_in_repo = file_name\n",
    "    \n",
    "    api.upload_file(\n",
    "        path_or_fileobj=local_file_path,        \n",
    "        path_in_repo=path_in_repo,\n",
    "        repo_id=repo_id,\n",
    "        repo_type=\"model\",\n",
    "    )\n",
    "    \n",
    "    print(f\"Uploaded {file_name} to {repo_id}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
