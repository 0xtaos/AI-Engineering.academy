{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8d0e81ed",
      "metadata": {},
      "source": [
        "<div align=\"center\">\n",
        "<h2>ReRanker RAG</h2>\n",
        "</div>\n",
        "\n",
        "<div align=\"center\">\n",
        "    <h3 ><a href=\"https://aiengineering.academy/\" target=\"_blank\">AI Engineering.academy</a></h3>\n",
        "    \n",
        "    \n",
        "</div>\n",
        "\n",
        "<div align=\"center\">\n",
        "<a href=\"https://aiengineering.academy/\" target=\"_blank\">\n",
        "<img src=\"https://raw.githubusercontent.com/adithya-s-k/AI-Engineering.academy/main/assets/banner.png\" alt=\"Ai Engineering. Academy\" width=\"50%\">\n",
        "</a>\n",
        "</div>\n",
        "\n",
        "\n",
        "<div align=\"center\">\n",
        "\n",
        "[![GitHub Stars](https://img.shields.io/github/stars/adithya-s-k/AI-Engineering.academy?style=social)](https://github.com/adithya-s-k/AI-Engineering.academy/stargazers)\n",
        "[![GitHub Forks](https://img.shields.io/github/forks/adithya-s-k/AI-Engineering.academy?style=social)](https://github.com/adithya-s-k/AI-Engineering.academy/network/members)\n",
        "[![GitHub Issues](https://img.shields.io/github/issues/adithya-s-k/AI-Engineering.academy)](https://github.com/adithya-s-k/AI-Engineering.academy/issues)\n",
        "[![GitHub Pull Requests](https://img.shields.io/github/issues-pr/adithya-s-k/AI-Engineering.academy)](https://github.com/adithya-s-k/AI-Engineering.academy/pulls)\n",
        "[![License](https://img.shields.io/github/license/adithya-s-k/AI-Engineering.academy)](https://github.com/adithya-s-k/AI-Engineering.academy/blob/main/LICENSE)\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce3c83b8",
      "metadata": {},
      "source": [
        "## Setting up the Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a4a5db8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install -U nest-asyncio\n",
        "# !pip install -U llama-index\n",
        "# !pip install -U llama-index-vector-stores-qdrant \n",
        "# !pip install -U llama-index-readers-file \n",
        "# !pip install -U llama-index-embeddings-fastembed \n",
        "# !pip install -U llama-index-llms-openai\n",
        "# !pip install -U llama-index-llms-groq\n",
        "# !pip install -U qdrant_client fastembed\n",
        "# !pip install -U python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "91b61e96-864c-4ed2-80d6-0ebfdbb57d5c",
      "metadata": {
        "id": "91b61e96-864c-4ed2-80d6-0ebfdbb57d5c"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "2c7131e9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard library imports\n",
        "import logging\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Third-party imports\n",
        "from dotenv import load_dotenv\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "# Qdrant client import\n",
        "import qdrant_client\n",
        "\n",
        "# LlamaIndex core imports\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
        "from llama_index.core import Settings\n",
        "\n",
        "# LlamaIndex vector store import\n",
        "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
        "\n",
        "# Embedding model imports\n",
        "from llama_index.embeddings.fastembed import FastEmbedEmbedding\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "\n",
        "# LLM import\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.llms.groq import Groq\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Get OpenAI API key from environment variables\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "GROK_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
        "\n",
        "# Setting up Base LLM\n",
        "Settings.llm = OpenAI(\n",
        "    model=\"gpt-4o-mini\", temperature=0.1, max_tokens=1024, streaming=True\n",
        ")\n",
        "\n",
        "# Settings.llm = Groq(model=\"llama3-70b-8192\" , api_key=GROK_API_KEY)\n",
        "\n",
        "# Set the embedding model\n",
        "# Option 1: Use FastEmbed with BAAI/bge-base-en-v1.5 model (default)\n",
        "# Settings.embed_model = FastEmbedEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")\n",
        "\n",
        "# Option 2: Use OpenAI's embedding model (commented out)\n",
        "# If you want to use OpenAI's embedding model, uncomment the following line:\n",
        "Settings.embed_model = OpenAIEmbedding(embed_batch_size=10, api_key=OPENAI_API_KEY)\n",
        "\n",
        "# Qdrant configuration (commented out)\n",
        "# If you're using Qdrant, uncomment and set these variables:\n",
        "# QDRANT_CLOUD_ENDPOINT = os.getenv(\"QDRANT_CLOUD_ENDPOINT\")\n",
        "# QDRANT_API_KEY = os.getenv(\"QDRANT_API_KEY\")\n",
        "\n",
        "# Note: Remember to add QDRANT_CLOUD_ENDPOINT and QDRANT_API_KEY to your .env file if using Qdrant Hosted version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "88344ea2-540c-4b46-8a66-ed78870cb80a",
      "metadata": {
        "id": "88344ea2-540c-4b46-8a66-ed78870cb80a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ”ƒ Loading Data\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:00<00:00, 19.22file/s]\n"
          ]
        }
      ],
      "source": [
        "# lets loading the documents using SimpleDirectoryReader\n",
        "\n",
        "print(\"ðŸ”ƒ Loading Data\")\n",
        "\n",
        "from llama_index.core import Document\n",
        "reader = SimpleDirectoryReader(\"../data/md/\" , recursive=True)\n",
        "documents = reader.load_data(show_progress=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05b3aafa",
      "metadata": {},
      "source": [
        "## Setting up Vector Database\n",
        "\n",
        "We will be using qDrant as the Vector database\n",
        "There are 4 ways to initialize qdrant \n",
        "\n",
        "1. Inmemory\n",
        "```python\n",
        "client = qdrant_client.QdrantClient(location=\":memory:\")\n",
        "```\n",
        "2. Disk\n",
        "```python\n",
        "client = qdrant_client.QdrantClient(path=\"./data\")\n",
        "```\n",
        "3. Self hosted or Docker\n",
        "```python\n",
        "\n",
        "client = qdrant_client.QdrantClient(\n",
        "    # url=\"http://<host>:<port>\"\n",
        "    host=\"localhost\",port=6333\n",
        ")\n",
        "```\n",
        "\n",
        "4. Qdrant cloud\n",
        "```python\n",
        "client = qdrant_client.QdrantClient(\n",
        "    url=QDRANT_CLOUD_ENDPOINT,\n",
        "    api_key=QDRANT_API_KEY,\n",
        ")\n",
        "```\n",
        "\n",
        "for this notebook we will be using qdrant cloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "e0d60472",
      "metadata": {},
      "outputs": [],
      "source": [
        "# creating a qdrant client instance\n",
        "\n",
        "client = qdrant_client.QdrantClient(\n",
        "    # you can use :memory: mode for fast and light-weight experiments,\n",
        "    # it does not require to have Qdrant deployed anywhere\n",
        "    # but requires qdrant-client >= 1.1.1\n",
        "    # location=\":memory:\"\n",
        "    # otherwise set Qdrant instance address with:\n",
        "    # url=QDRANT_CLOUD_ENDPOINT,\n",
        "    # otherwise set Qdrant instance with host and port:\n",
        "    host=\"localhost\",\n",
        "    port=6333\n",
        "    # set API KEY for Qdrant Cloud\n",
        "    # api_key=QDRANT_API_KEY,\n",
        "    # path=\"./db/\"\n",
        ")\n",
        "\n",
        "vector_store = QdrantVectorStore(client=client, collection_name=\"02_ReRanker_RAG\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4a677d6",
      "metadata": {},
      "source": [
        "### Ingest Data into vector DB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82475b3c",
      "metadata": {},
      "outputs": [],
      "source": [
        "## ingesting data into vector database\n",
        "\n",
        "## lets set up an ingestion pipeline\n",
        "\n",
        "from llama_index.core.node_parser import TokenTextSplitter\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.core.node_parser import MarkdownNodeParser\n",
        "from llama_index.core.node_parser import SemanticSplitterNodeParser\n",
        "from llama_index.core.ingestion import IngestionPipeline\n",
        "\n",
        "pipeline = IngestionPipeline(\n",
        "    transformations=[\n",
        "        # MarkdownNodeParser(include_metadata=True),\n",
        "        # TokenTextSplitter(chunk_size=500, chunk_overlap=20),\n",
        "        SentenceSplitter(chunk_size=1024, chunk_overlap=20),\n",
        "        # SemanticSplitterNodeParser(buffer_size=1, breakpoint_percentile_threshold=95 , embed_model=Settings.embed_model),\n",
        "        Settings.embed_model,\n",
        "    ],\n",
        "    vector_store=vector_store,\n",
        ")\n",
        "\n",
        "# Ingest directly into a vector db\n",
        "nodes = pipeline.run(documents=documents , show_progress=True)\n",
        "print(\"Number of chunks added to vector DB :\",len(nodes))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0a3c64f",
      "metadata": {},
      "source": [
        "## Setting Up Index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "da6fc12a",
      "metadata": {},
      "outputs": [],
      "source": [
        "index = VectorStoreIndex.from_vector_store(vector_store=vector_store)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e3d5f23-dfcd-458d-a9d3-dd66de0ab054",
      "metadata": {
        "id": "7e3d5f23-dfcd-458d-a9d3-dd66de0ab054"
      },
      "source": [
        "## Retrieval Comparisons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "47480c6d-8914-4562-a789-fd53a99a7afb",
      "metadata": {
        "id": "47480c6d-8914-4562-a789-fd53a99a7afb",
        "outputId": "04fa0ca7-1c01-4536-e5a3-86249ad0e283"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.retrievers import VectorIndexRetriever\n",
        "from llama_index.core import QueryBundle\n",
        "import pandas as pd\n",
        "from IPython.display import display, HTML\n",
        "from copy import deepcopy\n",
        "from llama_index.core.postprocessor import LLMRerank\n",
        "\n",
        "\n",
        "def get_retrieved_nodes(\n",
        "    query_str, vector_top_k=10, reranker_top_n=3, with_reranker=False\n",
        "):\n",
        "    query_bundle = QueryBundle(query_str)\n",
        "    # configure retriever\n",
        "    retriever = VectorIndexRetriever(\n",
        "        index=index,\n",
        "        similarity_top_k=vector_top_k,\n",
        "    )\n",
        "    retrieved_nodes = retriever.retrieve(query_bundle)\n",
        "\n",
        "    if with_reranker:\n",
        "        # configure reranker\n",
        "        reranker = LLMRerank(\n",
        "            choice_batch_size=5,\n",
        "            top_n=reranker_top_n,\n",
        "        )\n",
        "        retrieved_nodes = reranker.postprocess_nodes(\n",
        "            retrieved_nodes, query_bundle\n",
        "        )\n",
        "\n",
        "    return retrieved_nodes\n",
        "\n",
        "\n",
        "def pretty_print(df):\n",
        "    return display(HTML(df.to_html().replace(\"\\\\n\", \"<br>\")))\n",
        "\n",
        "\n",
        "def visualize_retrieved_nodes(nodes) -> None:\n",
        "    result_dicts = []\n",
        "    for node in nodes:\n",
        "        # node = deepcopy(node)\n",
        "        # node.node.metadata = None\n",
        "        node_text = node.node.get_text()\n",
        "        node_text = node_text.replace(\"\\n\", \" \")\n",
        "\n",
        "        result_dict = {\"Score\": node.score, \"Text\": node_text}\n",
        "        result_dicts.append(result_dict)\n",
        "\n",
        "    pretty_print(pd.DataFrame(result_dicts))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "b8bedc4f-444b-4233-9b72-728e3cfbe056",
      "metadata": {
        "id": "b8bedc4f-444b-4233-9b72-728e3cfbe056",
        "outputId": "c760c3b4-d9df-4122-c6a3-c0f2b83dbfa8"
      },
      "outputs": [],
      "source": [
        "new_nodes = get_retrieved_nodes(\n",
        "    \"What is Attention\", vector_top_k=5, with_reranker=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "e85e656b-9377-4640-a10d-a6655afd82bd",
      "metadata": {
        "id": "e85e656b-9377-4640-a10d-a6655afd82bd",
        "outputId": "86c28dde-995f-4a9d-e99d-0c224741cdc2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Score</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.828242</td>\n",
              "      <td>3.2 Attention  An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum. ---</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.828206</td>\n",
              "      <td>3.2 Attention  An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum. ---</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.825941</td>\n",
              "      <td>Attention Visualizations  It is this spirit that a majority of American governments have passed new laws since 2009.  Figure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb â€˜makingâ€™, completing the phrase â€˜making...more difficultâ€™. Attentions here shown only for the word â€˜makingâ€™. Different colors represent different heads. Best viewed in color.  Voting process more difficult. ---</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.816966</td>\n",
              "      <td>Attention Visualizations  It is this spirit that a majority of American governments have passed new laws since 2009.  Figure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6.  Attentions here shown only for the word â€˜makingâ€™. Different colors represent different heads. Best viewed in color. ---</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.808573</td>\n",
              "      <td>Figure 4  Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top: Full attentions for head 5. Bottom: Isolated attentions from just the word â€˜itsâ€™ for attention heads 5 and 6. Note that the attentions are very sharp for this word.  14 ---</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "visualize_retrieved_nodes(new_nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "8ba150e2-c4e7-4404-b8e1-1603c2b346d3",
      "metadata": {
        "id": "8ba150e2-c4e7-4404-b8e1-1603c2b346d3",
        "outputId": "35655788-a1b1-4452-f0ff-4b983aafadfa"
      },
      "outputs": [],
      "source": [
        "new_nodes = get_retrieved_nodes(\n",
        "    \"What is Attention\",\n",
        "    vector_top_k=20,\n",
        "    reranker_top_n=5,\n",
        "    with_reranker=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "7541606f-6424-470b-987c-a986ac0a7cf8",
      "metadata": {
        "id": "7541606f-6424-470b-987c-a986ac0a7cf8",
        "outputId": "9c514a6f-b6bd-4480-be17-702f5fe551c6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Score</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9.0</td>\n",
              "      <td>3.2 Attention  An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum. ---</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9.0</td>\n",
              "      <td>3.2 Attention  An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum. ---</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9.0</td>\n",
              "      <td>2 Background  The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions [12]. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2.  Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 27, 28, 22].  End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [34].  To the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [17, 18] and [9].</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9.0</td>\n",
              "      <td>3.2.1 Scaled Dot-Product Attention  We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the query with all keys, divide each by âˆšdk, and apply a softmax function to obtain the weights on the values.  In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q. The keys and values are also packed together into matrices K and V. We compute the matrix of outputs as:  Attention(Q, K, V) = softmax( âˆšdk Q KT ) V (1)  The two most commonly used attention functions are additive attention [2], and dot-product (multiplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor âˆš1/dk. Additive attention computes the compatibility function using a feed-forward network with a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.  While for small values of dk the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of dk [3]. We suspect that for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients4. To counteract this effect, we scale the dot products by âˆš1/dk.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9.0</td>\n",
              "      <td>1 Introduction  Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [35, 2, 5]. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [38, 24, 15].  Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states ht, as a function of the previous hidden state htâˆ’1 and the input for position t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has achieved significant improvements in computational efficiency through factorization tricks [21] and conditional computation [32], while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains.  Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms are used in conjunction with a recurrent network.  In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "visualize_retrieved_nodes(new_nodes)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35d84ea0",
      "metadata": {},
      "source": [
        "### LLM ReRanker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "afced092",
      "metadata": {},
      "outputs": [],
      "source": [
        "query_engine = index.as_query_engine(\n",
        "    similarity_top_k=10,\n",
        "    node_postprocessors=[\n",
        "        LLMRerank(\n",
        "            choice_batch_size=5,\n",
        "            top_n=2,\n",
        "        )\n",
        "    ],\n",
        "    response_mode=\"tree_summarize\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e183ed78",
      "metadata": {},
      "source": [
        "### Cohere ReRanker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d67ae8ac",
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install llama-index-postprocessor-cohere-rerank"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b20c0c27",
      "metadata": {},
      "outputs": [],
      "source": [
        "# from llama_index.postprocessor.cohere_rerank import CohereRerank\n",
        "\n",
        "# cohere_api_key = os.environ[\"COHERE_API_KEY\"]\n",
        "# cohere_rerank = CohereRerank(api_key=cohere_api_key, top_n=2)\n",
        "\n",
        "# query_engine = index.as_query_engine(\n",
        "#     similarity_top_k=10,\n",
        "#     node_postprocessors=[cohere_rerank],\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ef66d1c",
      "metadata": {},
      "source": [
        "### Colber ReRanker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1100a57",
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install -U llama-index-postprocessor-colbert-rerank"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "881f71e5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# from llama_index.postprocessor.colbert_rerank import ColbertRerank\n",
        "\n",
        "# colbert_reranker = ColbertRerank(\n",
        "#     top_n=5,\n",
        "#     model=\"colbert-ir/colbertv2.0\",\n",
        "#     tokenizer=\"colbert-ir/colbertv2.0\",\n",
        "#     keep_retrieval_score=True,\n",
        "# )\n",
        "\n",
        "# query_engine = index.as_query_engine(\n",
        "#     similarity_top_k=10,\n",
        "#     node_postprocessors=[colbert_reranker],\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec2512b0",
      "metadata": {},
      "source": [
        "### Flag Embedding ReRanker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cd4e239",
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install llama-index-postprocessor-flag-embedding-reranker\n",
        "# !pip install git+https://github.com/FlagOpen/FlagEmbedding.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e1990e34",
      "metadata": {},
      "outputs": [],
      "source": [
        "# from llama_index.postprocessor.flag_embedding_reranker import (\n",
        "#     FlagEmbeddingReranker,\n",
        "# )\n",
        "\n",
        "# rerank = FlagEmbeddingReranker(model=\"BAAI/bge-reranker-large\", top_n=5)\n",
        "\n",
        "# query_engine = index.as_query_engine(\n",
        "#     similarity_top_k=10, node_postprocessors=[rerank]\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9fa5bca",
      "metadata": {},
      "source": [
        "### Sentence Transformer ReRanker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b352ec4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install llama-index-embeddings-huggingface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66e9afc4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# from llama_index.core.postprocessor import SentenceTransformerRerank\n",
        "\n",
        "# rerank = SentenceTransformerRerank(\n",
        "#     model=\"cross-encoder/ms-marco-MiniLM-L-2-v2\", top_n=3\n",
        "# )\n",
        "\n",
        "# query_engine = index.as_query_engine(\n",
        "#     similarity_top_k=10, node_postprocessors=[rerank]\n",
        "# )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff5bbf1b",
      "metadata": {},
      "source": [
        "### Set Up Query Engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "b358c704",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Attention can be described as a function that maps a query and a set of key-value pairs to an output. In this process, the query, keys, values, and output are all represented as vectors, and the output is computed as a weighted sum of these vectors."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "response = query_engine.query(\n",
        "    \"What is Attention\"\n",
        ")\n",
        "\n",
        "display(Markdown(str(response)))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "basic-rag",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
