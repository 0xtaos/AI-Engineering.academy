{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fireship GPT\n",
    "an attempt at making an LLM emulate the tone, pacing, and content style of [Fireship](https://www.youtube.com/@Fireship) by fine-tuning them on curated datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GenerationConfig, TextIteratorStreamer, TextStreamer , AutoModelForCausalLM , AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM .from_pretrained('AdithyaSK/Fireship-GPT-v1')\n",
    "tokenizer = AutoTokenizer.from_pretrained('AdithyaSK/Fireship-GPT-v1')\n",
    "model = model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input the title of the video\n",
    "video_title = \"Rust in 100 seconds\"\n",
    "\n",
    "# input the a small summary of the video\n",
    "video_summary = \"\"\n",
    "\n",
    "\n",
    "prompt = f\"\"\"\n",
    "[INST]\n",
    "You are youtuber called Fireship you make engaging high-intensity and entertaining coding tutorials and tech news. \n",
    "you covers a wide range of topics relevant to programmers, aiming to help them learn and improve their skills quickly.\n",
    "\n",
    "Given the title of the video : {video_title} \n",
    "and a small summary : {video_summary}\n",
    "[/INST]\n",
    "\n",
    "Generate the video : \n",
    "\"\"\"\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    repetition_penalty=1.1,\n",
    "    max_new_tokens=1024,\n",
    "    temperature=0.9,\n",
    "    top_p=0.95,\n",
    "    top_k=40,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    do_sample=True,\n",
    "    use_cache=True,\n",
    "    return_dict_in_generate=True,\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False,\n",
    "    output_scores=False,\n",
    ")\n",
    "streamer = TextStreamer(tokenizer)\n",
    "batch = tokenizer(str(prompt.strip()), return_tensors=\"pt\", add_special_tokens=True)\n",
    "generated = model.generate(\n",
    "    inputs=batch[\"input_ids\"].to(\"cuda\"),\n",
    "    generation_config=generation_config,\n",
    "    streamer=streamer,\n",
    ")\n",
    "print(tokenizer.decode(generated[\"sequences\"].cpu().tolist()[0]))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
